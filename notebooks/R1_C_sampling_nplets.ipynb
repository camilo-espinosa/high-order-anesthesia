{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e96cf4",
   "metadata": {},
   "source": [
    "**Note**: This notebook should be run from the `high-order-anesthesia` folder to ensure the correct imports and file paths are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee8453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in: high-order-anesthesia\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "def ensure_project_root(target_name: str = \"high-order-anesthesia\") -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if cwd.name == target_name:\n",
    "        return cwd\n",
    "    for parent in cwd.parents:\n",
    "        if parent.name == target_name:\n",
    "            os.chdir(parent)\n",
    "            return parent\n",
    "    raise RuntimeError(\n",
    "        f\"Could not find '{target_name}' in current path or parents. \"\n",
    "        f\"Please run the notebook from inside the project.\"\n",
    "    )\n",
    "ROOT = ensure_project_root(\"high-order-anesthesia\")\n",
    "print(f\"Now in: {ROOT.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c498039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c5288",
   "metadata": {},
   "source": [
    "#### Custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hoi_anesthesia.io import load_covariance_dict, save_results\n",
    "from src.hoi_anesthesia.utils import O_PR_AUC, generate_X,analyze_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2078907",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "data_path = \"results\"\n",
    "all_covs = load_covariance_dict(f\"{data_path}/covariance_matrices_gc.h5\")\n",
    "N = 82\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Np_max = 500_000\n",
    "M = 50_000_000\n",
    "R = 82\n",
    "batch_size = 1000\n",
    "p = 0.05  # 5% of the total number of nplets\n",
    "eval_func = O_PR_AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cac6d",
   "metadata": {},
   "source": [
    "MA: order 4 (synergy brain), order 7 (redundancy brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ab737",
   "metadata": {},
   "outputs": [],
   "source": [
    "conscious_states = {\"MA\": [\"MA_awake\"]}\n",
    "nonresponsive_states = {\n",
    "    \"MA\": [\"deep_propofol\", \"ketamine\", \"moderate_propofol\", \"ts_selv2\", \"ts_selv4\"],\n",
    "}\n",
    "X_tensor, n_c, n_nr = generate_X(conscious_states, nonresponsive_states, all_covs)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "for k in [4,7]:\n",
    "    results = {\"MA\": {}}\n",
    "    tail_size = int(min(math.comb(R, k) * p, Np_max))\n",
    "    print(\n",
    "        f\"MA, order {k} | tail size: {tail_size:.0f} (of {math.comb(R, k):.0f} combinations)\"\n",
    "    )\n",
    "    top_c, top_nr = analyze_order(\n",
    "        X_tensor=X_tensor,\n",
    "        n_c=n_c,\n",
    "        k=k,\n",
    "        M=M,\n",
    "        Np=tail_size,\n",
    "        batch_size=batch_size,\n",
    "        R=R,\n",
    "        device=device,\n",
    "        eval_fc=eval_func,\n",
    "    )\n",
    "    # top_c: high O -> C (best PR with C positive)\n",
    "    # top_nr: high O -> NR (best PR with NR positive)\n",
    "    results[\"MA\"][k] = {\n",
    "        \"C_positive\": top_c,\n",
    "        \"NR_positive\": top_nr,\n",
    "    }\n",
    "    save_results(results, f\"{results_path}/nplet_tails_PRAUC_MA_{k}.pkl.gz\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24f633",
   "metadata": {},
   "source": [
    "DBS: order 3 (synergy brain), order 9 (redundancy brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68efe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "conscious_states = {\"DBS\": [\"DBS_awake\", \"ts_on_5V\"]}\n",
    "nonresponsive_states = {\n",
    "    \"DBS\": [\"ts_off\", \"ts_on_3V_control\", \"ts_on_5V_control\"],\n",
    "}\n",
    "X_tensor, n_c, n_nr = generate_X(conscious_states, nonresponsive_states, all_covs)\n",
    "\n",
    "for k in [3,9]:\n",
    "    results = {\"DBS\": {}}\n",
    "    tail_size = int(min(math.comb(R, k) * p, Np_max))\n",
    "    print(\n",
    "        f\"DBS, order {k} | tail size: {tail_size:.0f} (of {math.comb(R, k):.0f} combinations)\"\n",
    "    )\n",
    "    top_c, top_nr = analyze_order(\n",
    "        X_tensor=X_tensor,\n",
    "        n_c=n_c,\n",
    "        k=k,\n",
    "        M=M,\n",
    "        Np=tail_size,\n",
    "        batch_size=batch_size,\n",
    "        R=R,\n",
    "        device=device,\n",
    "        eval_fc=eval_func,\n",
    "    )\n",
    "    results[\"DBS\"][k] = {\n",
    "        \"C_positive\": top_c,\n",
    "        \"NR_positive\": top_nr,\n",
    "    }\n",
    "    save_results(results, f\"{results_path}/nplet_tails_PRAUC_DBS_{k}.pkl.gz\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def attach_delta_O_to_tail(\n",
    "    X_tensor: torch.Tensor,\n",
    "    n_c: int,\n",
    "    tail: List[Tuple[float, Tuple[int, ...]]],\n",
    "    k: int,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 2048,\n",
    ") -> List[Tuple[float, float, Tuple[int, ...]]]:\n",
    "    \"\"\"\n",
    "    Given a tail of (PR_AUC, nplet) and subject covariances X_tensor,\n",
    "    compute O for those n-plets, then ΔO = mean(O_C) - mean(O_NR),\n",
    "    and return (PR_AUC, ΔO, nplet) for each entry, in the same order.\n",
    "    \"\"\"\n",
    "    if len(tail) == 0:\n",
    "        return []\n",
    "\n",
    "    # Unpack\n",
    "    pr_scores, nplets = zip(\n",
    "        *tail\n",
    "    )  # pr_scores: tuple[float], nplets: tuple[tuple[int,...]]\n",
    "    pr_scores = np.array(pr_scores, dtype=float)\n",
    "    nplets_arr = torch.tensor(nplets, dtype=torch.long)  # (B, k)\n",
    "\n",
    "    X_tensor = X_tensor.to(device=device, dtype=torch.float64)\n",
    "\n",
    "    B = nplets_arr.shape[0]\n",
    "    delta_O_all = np.empty(B, dtype=float)\n",
    "\n",
    "    for start in tqdm(\n",
    "        range(0, B, batch_size),\n",
    "        desc=f\"Computing ΔO for tail (k={k}, B={B})\",\n",
    "    ):\n",
    "        end = min(start + batch_size, B)\n",
    "        nplets_batch = nplets_arr[start:end].to(device=device)\n",
    "\n",
    "        # Compute measures\n",
    "        measures = nplets_measures(\n",
    "            X=X_tensor,\n",
    "            covmat_precomputed=True,\n",
    "            nplets=nplets_batch,\n",
    "            device=device,\n",
    "            verbose=logging.WARNING,\n",
    "        )\n",
    "        # measures[..., 2] -> O-information\n",
    "        O_vals = (\n",
    "            measures[..., 2].detach().cpu().numpy()\n",
    "        )  # shape either (b, N) or (N, b)\n",
    "\n",
    "        # Ensure O_vals is (N_subjects, b)\n",
    "        if O_vals.shape[0] != X_tensor.shape[0]:\n",
    "            O_vals = O_vals.T  # now (N_subjects, b)\n",
    "\n",
    "        # Use your existing delta_O function\n",
    "        delta_O_batch, _ = delta_O(O_vals, n_c)  # (b,)\n",
    "        delta_O_all[start:end] = delta_O_batch\n",
    "\n",
    "    # Build new tail with ΔO attached\n",
    "    tail_with_delta = [\n",
    "        (float(pr_scores[i]), float(delta_O_all[i]), nplets[i]) for i in range(B)\n",
    "    ]\n",
    "    return tail_with_delta\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2 main: load all tails, merge, compute ΔO, save merged dict\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Load step-1 tails and merge into a single dict\n",
    "merged_results = {\"MA\": {}, \"DBS\": {}}\n",
    "\n",
    "# MA: orders 4 (redundant NRpos) and 7 (redundant Cpos)\n",
    "for k in [4, 7]:\n",
    "    path = f\"{results_path}/nplet_tails_PRAUC_MA_{k}.pkl.gz\"\n",
    "    print(f\"Loading MA order {k} tails from: {path}\")\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        res = pickle.load(f)  # {\"MA\": {k: {\"C_positive\": [...], \"NR_positive\": [...]}}}\n",
    "    merged_results[\"MA\"][k] = res[\"MA\"][k]\n",
    "\n",
    "# DBS: orders 3 (synergy NRpos) and 9 (redundant Cpos)\n",
    "for k in [3, 9]:\n",
    "    path = f\"{results_path}/nplet_tails_PRAUC_DBS_{k}.pkl.gz\"\n",
    "    print(f\"Loading DBS order {k} tails from: {path}\")\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        res = pickle.load(\n",
    "            f\n",
    "        )  # {\"DBS\": {k: {\"C_positive\": [...], \"NR_positive\": [...]}}}\n",
    "    merged_results[\"DBS\"][k] = res[\"DBS\"][k]\n",
    "\n",
    "# 2) Rebuild X_tensor and n_c for each dataset (same as step 1)\n",
    "\n",
    "\n",
    "# 3) For each dataset/order/tail, attach ΔO\n",
    "\n",
    "for dataset in [\"MA\", \"DBS\"]:\n",
    "    print(f\"\\n=== Attaching ΔO for dataset: {dataset} ===\")\n",
    "    X_tensor, n_c = build_X_for_dataset(dataset)\n",
    "    X_tensor = X_tensor.to(device=device, dtype=torch.float64)\n",
    "\n",
    "    for k, tails in merged_results[dataset].items():\n",
    "        print(f\"  Order k={k}\")\n",
    "\n",
    "        for key in [\"C_positive\", \"NR_positive\"]:\n",
    "            tail = tails.get(key, [])\n",
    "            if not tail:\n",
    "                merged_results[dataset][k][key] = []\n",
    "                continue\n",
    "\n",
    "            print(f\"    Tail: {key}, n={len(tail)}\")\n",
    "            tail_with_delta = attach_delta_O_to_tail(\n",
    "                X_tensor=X_tensor,\n",
    "                n_c=n_c,\n",
    "                tail=tail,\n",
    "                k=k,\n",
    "                device=device,\n",
    "                batch_size=2048,\n",
    "            )\n",
    "            # Now each element: (PR_AUC, ΔO, nplet_tuple)\n",
    "            merged_results[dataset][k][key] = tail_with_delta\n",
    "\n",
    "merged_results[dataset][k][key][0]\n",
    "\n",
    "# 4) Save merged dict with ΔO attached for all datasets/orders/tails\n",
    "out_path = f\"{results_path}/B_2_nplet_tails_PRAUC_with_deltaO_ALL.pkl.gz\"\n",
    "print(f\"\\nSaving merged results with ΔO to: {out_path}\")\n",
    "save_results(merged_results, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "merged_path = f\"{results_path}/B_2_nplet_tails_PRAUC_with_deltaO_ALL.pkl.gz\"\n",
    "print(f\"Loading merged tails (with ΔO) from: {merged_path}\")\n",
    "with gzip.open(merged_path, \"rb\") as f:\n",
    "    merged_results = pickle.load(f)\n",
    "    # structure:\n",
    "    # merged_results[dataset][k][\"C_positive\"] = [(pr_auc, delta_O, nplet), ...]\n",
    "    # merged_results[dataset][k][\"NR_positive\"] = [(pr_auc, delta_O, nplet), ...]\n",
    "\n",
    "merged_results[\"MA\"][4][\"NR_positive\"][0]\n",
    "merged_results[\"MA\"][7][\"C_positive\"][0]\n",
    "merged_results[\"DBS\"][3][\"NR_positive\"][0]\n",
    "merged_results[\"DBS\"][9][\"C_positive\"][0]\n",
    "\n",
    "\n",
    "def build_region_maps_for_tail(\n",
    "    tail_with_delta,\n",
    "    mode: str,\n",
    "    R: int = 82,\n",
    "    delta_eps: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    tail_with_delta: list of (pr_auc, delta_O, nplet_tuple)\n",
    "    mode: \"C_positive\" -> keep ΔO >  delta_eps\n",
    "          \"NR_positive\" -> keep ΔO < -delta_eps (or < 0 if delta_eps == 0)\n",
    "\n",
    "    Returns:\n",
    "        region_counts       : (R,) integer count of how many n-plets include each region\n",
    "        region_counts_prop  : (R,) counts divided by total # of kept n-plets\n",
    "        region_counts_z     : (R,) z-scored counts across regions (for plotting)\n",
    "    \"\"\"\n",
    "    if len(tail_with_delta) == 0:\n",
    "        return (\n",
    "            np.zeros(R, dtype=int),\n",
    "            np.zeros(R, dtype=float),\n",
    "            np.zeros(R, dtype=float),\n",
    "        )\n",
    "\n",
    "    # 1) Filter n-plets by ΔO sign according to mode\n",
    "    nplet_list = []\n",
    "\n",
    "    for pr_auc, delta_O_val, nplet in tail_with_delta:\n",
    "        if mode == \"C_positive\":\n",
    "            if delta_O_val > delta_eps:\n",
    "                nplet_list.append(nplet)\n",
    "        elif mode == \"NR_positive\":\n",
    "            if delta_O_val < -delta_eps if delta_eps > 0 else delta_O_val < 0.0:\n",
    "                nplet_list.append(nplet)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    N_tail_filtered = len(nplet_list)\n",
    "    print(f\"      After ΔO filtering ({mode}): {N_tail_filtered} n-plets\")\n",
    "\n",
    "    if N_tail_filtered == 0:\n",
    "        return (\n",
    "            np.zeros(R, dtype=int),\n",
    "            np.zeros(R, dtype=float),\n",
    "            np.zeros(R, dtype=float),\n",
    "        )\n",
    "\n",
    "    # 2) Compute region participation (raw counts)\n",
    "    region_counts = np.zeros(R, dtype=int)\n",
    "\n",
    "    for nplet in nplet_list:\n",
    "        for r in nplet:\n",
    "            region_counts[r] += 1\n",
    "\n",
    "    # 3) Counts as proportion of all kept n-plets\n",
    "    region_counts_prop = region_counts.astype(float) / float(N_tail_filtered)\n",
    "    region_counts_percent = (region_counts.astype(float) / float(N_tail_filtered)) * 100\n",
    "\n",
    "    # 4) z-score counts across regions (for plotting)\n",
    "    mu = region_counts.mean()\n",
    "    sigma = region_counts.std()\n",
    "    if sigma > 0:\n",
    "        region_counts_z = (region_counts - mu) / sigma\n",
    "    else:\n",
    "        region_counts_z = np.zeros_like(region_counts, dtype=float)\n",
    "\n",
    "    return region_counts, region_counts_prop, region_counts_z, region_counts_percent\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build region maps for all dataset / order / tail\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "region_maps = {\n",
    "    \"MA\": {},\n",
    "    \"DBS\": {},\n",
    "}\n",
    "\n",
    "for dataset in [\"MA\", \"DBS\"]:\n",
    "    print(f\"\\n=== Building region maps for dataset: {dataset} ===\")\n",
    "\n",
    "    for k, tails in merged_results[dataset].items():\n",
    "        print(f\"  Order k={k}\")\n",
    "        region_maps[dataset][k] = {}\n",
    "\n",
    "        for key in [\"C_positive\", \"NR_positive\"]:\n",
    "            tail_with_delta = tails.get(key, [])\n",
    "            # tail_with_delta[0]\n",
    "            print(f\"    Tail: {key}, n_raw={len(tail_with_delta)}\")\n",
    "\n",
    "            (\n",
    "                region_counts,\n",
    "                region_counts_prop,\n",
    "                region_counts_z,\n",
    "                region_counts_percent,\n",
    "            ) = build_region_maps_for_tail(\n",
    "                tail_with_delta=tail_with_delta,\n",
    "                mode=key,  # \"C_positive\" or \"NR_positive\"\n",
    "                R=R,\n",
    "                delta_eps=0.0,  # you can set >0 to be stricter on ΔO\n",
    "            )\n",
    "\n",
    "            region_maps[dataset][k][key] = {\n",
    "                \"region_counts\": region_counts,\n",
    "                \"region_counts_prop\": region_counts_prop,\n",
    "                \"region_counts_z\": region_counts_z,\n",
    "                \"region_counts_percent\": region_counts_percent,\n",
    "            }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Save region_maps to disk for plotting\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "out_maps_path = f\"{results_path}/B_3_region_maps_PRAUC_deltaO.pkl.gz\"\n",
    "print(f\"\\nSaving region maps to: {out_maps_path}\")\n",
    "with gzip.open(out_maps_path, \"wb\") as f:\n",
    "    pickle.dump(region_maps, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 10  # number of max indices you want\n",
    "arr = region_maps[\"MA\"][4][\"NR_positive\"][\"region_counts\"]\n",
    "idx = np.argpartition(arr, -X)[-X:]  # gets the indices of the top X values (unordered)\n",
    "idx_sorted = idx[\n",
    "    np.argsort(arr[idx])[::-1]\n",
    "]  # optional: sort them by value (descending)\n",
    "\n",
    "print(idx_sorted)\n",
    "\n",
    "# plt.hist(region_maps[\"MA\"][4]['NR_positive']['region_counts'],bins=100)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".high_order_anesthesia_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
