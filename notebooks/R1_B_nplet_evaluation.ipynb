{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980afe87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "from thoi.measures.gaussian_copula import nplets_measures\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "results_path = \"C:/CAMILO/Brain_Entropy/RESULTS\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def print_time(t_i, t_f):\n",
    "    elapsed_time_seconds = t_f - t_i\n",
    "    hours = int(elapsed_time_seconds // 3600)\n",
    "    minutes = int((elapsed_time_seconds % 3600) // 60)\n",
    "    seconds = int(elapsed_time_seconds % 60)\n",
    "    print(\"Elapsed time: {:02d}:{:02d}:{:02d}\".format(hours, minutes, seconds))\n",
    "\n",
    "\n",
    "def load_covariance_dict(filepath, lazy=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the .h5 file.\n",
    "    lazy : bool\n",
    "        If False (default)  → load every dataset into RAM (returns plain NumPy arrays).\n",
    "        If True             → return h5py.Dataset handles (zero-copy); file must stay open.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    covs : dict\n",
    "        Two-level dict: covs[dataset][state] = ndarray | h5py.Dataset\n",
    "    \"\"\"\n",
    "    covs = defaultdict(dict)\n",
    "\n",
    "    if lazy:\n",
    "        # keep file handle alive by attaching it to the dictionary itself\n",
    "        h5f = h5py.File(filepath, \"r\")\n",
    "        covs[\"_h5file\"] = h5f  # so GC won't close it\n",
    "        for dataset in h5f:\n",
    "            for state in h5f[dataset]:\n",
    "                covs[dataset][state] = h5f[dataset][state]  # h5py.Dataset\n",
    "    else:\n",
    "        with h5py.File(filepath, \"r\") as h5f:\n",
    "            for dataset in h5f:\n",
    "                for state in h5f[dataset]:\n",
    "                    covs[dataset][state] = h5f[dataset][state][:]\n",
    "    return covs\n",
    "\n",
    "\n",
    "def evaluate_nplet_batched(\n",
    "    idx,\n",
    "    all_covs,\n",
    "    conscious_states,\n",
    "    nonresponsive_states,\n",
    "    selected_dataset,\n",
    "    optimal_nplet,\n",
    "    state_c,  # tuple-like (ds, st) for discovery in conscious set\n",
    "    state_nr,  # tuple-like (ds, st) for discovery in nonresponsive set\n",
    "    subject_c,  # index to skip in conscious discovery pair\n",
    "    subject_nr,  # index to skip in nonresponsive discovery pair\n",
    "    device,\n",
    "):\n",
    "\n",
    "    # selected_dataset = result_name\n",
    "    # optimal_nplet =ast.literal_eval(row[\"optimal_nplet\"])\n",
    "    # state_c = row[\"state_c\"]  # tuple-like (ds, st) for discovery in conscious set\n",
    "    # state_nr = row[\"state_nr\"]  # tuple-like (ds, st) for discovery in nonresponsive set\n",
    "    # subject_c = row[\"subject_c\"]  # index to skip in conscious discovery pair\n",
    "    # subject_nr = row[\"subject_nr\"]  # index to skip in nonresponsive discovery pair\n",
    "    \"\"\"\n",
    "    Batched evaluation of a single n-plet:\n",
    "      - Computes TC, DTC, O, S (indices 0..3) in one pass.\n",
    "      - Processes all subjects per condition in a single call to nplets_measures\n",
    "        by passing a list of covariance matrices (covmat_precomputed=True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns: measure, F_score, Cohen_d, AUROC, PR_AUC\n",
    "    \"\"\"\n",
    "    measure_list = [\"TC\", \"DTC\", \"O\", \"S\", \"norm_O\"]\n",
    "\n",
    "    # --- Gather covariance matrices per condition (skip discovery subject)\n",
    "    covs_conscious = []\n",
    "    for st in conscious_states[selected_dataset]:\n",
    "        covs = all_covs[selected_dataset][st]  # shape: (n_subj, N, N), covariances\n",
    "        for subj_idx in range(covs.shape[0]):\n",
    "            if st == state_c and subj_idx == subject_c:\n",
    "                continue\n",
    "            covs_conscious.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    covs_nonresp = []\n",
    "    for st in nonresponsive_states[selected_dataset]:\n",
    "        covs = all_covs[selected_dataset][st]\n",
    "        for subj_idx in range(covs.shape[0]):\n",
    "            if st == state_nr and subj_idx == subject_nr:\n",
    "                continue\n",
    "            covs_nonresp.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    n_c = len(covs_conscious)\n",
    "    n_nr = len(covs_nonresp)\n",
    "\n",
    "    if n_c == 0 or n_nr == 0:\n",
    "        raise ValueError(\n",
    "            \"Empty group after skipping discovery subject(s). Check inputs.\"\n",
    "        )\n",
    "\n",
    "    # --- Single batched call over ALL subjects (conscious first, then nonresponsive)\n",
    "    X_list = covs_conscious + covs_nonresp  # list of (N,N)\n",
    "    X_array = np.array(X_list)\n",
    "    X_tensor = torch.tensor(X_array)\n",
    "    measures = nplets_measures(\n",
    "        X_tensor,\n",
    "        nplets=[optimal_nplet],  # single n-plet\n",
    "        covmat_precomputed=True,\n",
    "        T=None,  # keep same behavior as your original code\n",
    "        device=device,\n",
    "        verbose=logging.WARNING,\n",
    "    )\n",
    "    # measures shape: (1, D, 4) where D = n_c + n_nr\n",
    "    vals = measures[0, :, :4].detach().cpu().numpy()  # (D, 4)\n",
    "\n",
    "    ratio = vals[:, 2] / vals[:, 3]  # shape (D,)\n",
    "    vals = np.column_stack((vals, ratio))  # shape (D,5)\n",
    "    # --- Labels: 1 for conscious, 0 for nonresponsive\n",
    "    y = np.concatenate([np.ones(n_c, dtype=int), np.zeros(n_nr, dtype=int)])  # (D,)\n",
    "\n",
    "    # ANOVA F-score across the 4 columns\n",
    "    F_vals, _ = f_classif(vals, y)  # shape (4,)\n",
    "    for j in range(4):\n",
    "        if np.allclose(vals[:, j].var(), 0):\n",
    "            F_vals[j] = 0.0\n",
    "    # AUROC & PR AUC per column\n",
    "    pr_aucs = []\n",
    "    neg_pr_aucs = []\n",
    "    y_neg = 1 - y\n",
    "    for j in range(vals.shape[1]):\n",
    "        xj = vals[:, j]\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(y, xj))\n",
    "            neg_pr_aucs.append(average_precision_score(y, -xj))\n",
    "        except ValueError:\n",
    "            pr_aucs.append(np.nan)\n",
    "            neg_pr_aucs.append(np.nan)\n",
    "\n",
    "    out_list = []\n",
    "    for jdx, measure_ in enumerate(measure_list):\n",
    "        out_list.append(\n",
    "            {\n",
    "                \"row_idx\": idx,\n",
    "                \"measure\": measure_,\n",
    "                \"F_score\": F_vals[jdx],\n",
    "                \"PR_AUC\": pr_aucs[jdx],\n",
    "                \"PR_AUC_inv\": neg_pr_aucs[jdx],\n",
    "            }\n",
    "        )\n",
    "    return out_list\n",
    "\n",
    "\n",
    "def evaluate_nplet_batched_both_datasets(\n",
    "    idx,\n",
    "    all_covs,\n",
    "    conscious_states,\n",
    "    nonresponsive_states,\n",
    "    optimal_nplet,\n",
    "    state_c,  # tuple-like (ds, st) for discovery in conscious set\n",
    "    state_nr,  # tuple-like (ds, st) for discovery in nonresponsive set\n",
    "    subject_c,  # index to skip in conscious discovery pair\n",
    "    subject_nr,  # index to skip in nonresponsive discovery pair\n",
    "    device,\n",
    "):\n",
    "\n",
    "    # selected_dataset = result_name\n",
    "    # optimal_nplet =ast.literal_eval(row[\"optimal_nplet\"])\n",
    "    # state_c = row[\"state_c\"]  # tuple-like (ds, st) for discovery in conscious set\n",
    "    # state_nr = row[\"state_nr\"]  # tuple-like (ds, st) for discovery in nonresponsive set\n",
    "    # subject_c = row[\"subject_c\"]  # index to skip in conscious discovery pair\n",
    "    # subject_nr = row[\"subject_nr\"]  # index to skip in nonresponsive discovery pair\n",
    "    \"\"\"\n",
    "    Batched evaluation of a single n-plet:\n",
    "      - Computes TC, DTC, O, S (indices 0..3) in one pass.\n",
    "      - Processes all subjects per condition in a single call to nplets_measures\n",
    "        by passing a list of covariance matrices (covmat_precomputed=True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns: measure, F_score, Cohen_d, AUROC, PR_AUC\n",
    "    \"\"\"\n",
    "    measure_list = [\"TC\", \"DTC\", \"O\", \"S\", \"norm_O\"]\n",
    "\n",
    "    # --- Gather covariance matrices per condition (skip discovery subject)\n",
    "    covs_conscious = []\n",
    "    for ds, states in conscious_states.items():\n",
    "        for st in states:\n",
    "            covs = all_covs[ds][st]  # shape: (n_subj, N, N), covariances\n",
    "            for subj_idx in range(covs.shape[0]):\n",
    "                if st == state_c and subj_idx == subject_c:\n",
    "                    continue\n",
    "                covs_conscious.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    covs_nonresp = []\n",
    "    for ds, states in nonresponsive_states.items():\n",
    "        for st in states:\n",
    "            covs = all_covs[ds][st]\n",
    "            for subj_idx in range(covs.shape[0]):\n",
    "                if st == state_nr and subj_idx == subject_nr:\n",
    "                    continue\n",
    "                covs_nonresp.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    n_c = len(covs_conscious)\n",
    "    n_nr = len(covs_nonresp)\n",
    "\n",
    "    if n_c == 0 or n_nr == 0:\n",
    "        raise ValueError(\n",
    "            \"Empty group after skipping discovery subject(s). Check inputs.\"\n",
    "        )\n",
    "\n",
    "    # --- Single batched call over ALL subjects (conscious first, then nonresponsive)\n",
    "    X_list = covs_conscious + covs_nonresp  # list of (N,N)\n",
    "    X_array = np.array(X_list)\n",
    "    X_tensor = torch.tensor(X_array)\n",
    "    measures = nplets_measures(\n",
    "        X_tensor,\n",
    "        nplets=[optimal_nplet],  # single n-plet\n",
    "        covmat_precomputed=True,\n",
    "        T=None,  # keep same behavior as your original code\n",
    "        device=device,\n",
    "        verbose=logging.WARNING,\n",
    "    )\n",
    "    # measures shape: (1, D, 4) where D = n_c + n_nr\n",
    "    vals = measures[0, :, :4].detach().cpu().numpy()  # (D, 4)\n",
    "    ratio = vals[:, 2] / vals[:, 3]  # shape (D,)\n",
    "    vals = np.column_stack((vals, ratio))  # shape (D,5)\n",
    "    # --- Labels: 1 for conscious, 0 for nonresponsive\n",
    "    y = np.concatenate([np.ones(n_c, dtype=int), np.zeros(n_nr, dtype=int)])  # (D,)\n",
    "\n",
    "    # ANOVA F-score across the 4 columns\n",
    "    F_vals, _ = f_classif(vals, y)  # shape (4,)\n",
    "    for j in range(4):\n",
    "        if np.allclose(vals[:, j].var(), 0):\n",
    "            F_vals[j] = 0.0\n",
    "    # AUROC & PR AUC per column\n",
    "    pr_aucs = []\n",
    "    neg_pr_aucs = []\n",
    "    for j in range(vals.shape[1]):\n",
    "        xj = vals[:, j]\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(y, xj))\n",
    "            neg_pr_aucs.append(average_precision_score(y, -xj))\n",
    "        except ValueError:\n",
    "            pr_aucs.append(np.nan)\n",
    "            neg_pr_aucs.append(np.nan)\n",
    "\n",
    "    out_list = []\n",
    "    for jdx, measure_ in enumerate(measure_list):\n",
    "        out_list.append(\n",
    "            {\n",
    "                \"row_idx\": idx,\n",
    "                \"measure\": measure_,\n",
    "                \"F_score\": F_vals[jdx],\n",
    "                \"PR_AUC\": pr_aucs[jdx],\n",
    "                \"PR_AUC_inv\": neg_pr_aucs[jdx],\n",
    "            }\n",
    "        )\n",
    "    return out_list\n",
    "\n",
    "\n",
    "def evaluate_nplet_batched_FC(\n",
    "    idx,\n",
    "    all_covs,\n",
    "    conscious_states,\n",
    "    nonresponsive_states,\n",
    "    selected_dataset,\n",
    "    optimal_nplet,\n",
    "    state_c,  # tuple-like (ds, st) for discovery in conscious set\n",
    "    state_nr,  # tuple-like (ds, st) for discovery in nonresponsive set\n",
    "    subject_c,  # index to skip in conscious discovery pair\n",
    "    subject_nr,  # index to skip in nonresponsive discovery pair\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Batched evaluation of a single n-plet:\n",
    "      - Computes TC, DTC, O, S, norm_O, FC_mean_z in one pass.\n",
    "      - Processes all subjects per condition in a single call to nplets_measures\n",
    "        by passing a list of covariance matrices (covmat_precomputed=True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts, one per measure, with:\n",
    "        row_idx, measure, F_score, PR_AUC, PR_AUC_inv\n",
    "    \"\"\"\n",
    "    # HOI measures + classical FC\n",
    "    measure_list = [\"TC\", \"DTC\", \"O\", \"S\", \"norm_O\", \"FC_mean_z\"]\n",
    "\n",
    "    # --- Gather covariance matrices per condition (skip discovery subject)\n",
    "    covs_conscious = []\n",
    "    for st in conscious_states[selected_dataset]:\n",
    "        covs = all_covs[selected_dataset][st]  # (n_subj, N, N)\n",
    "        for subj_idx in range(covs.shape[0]):\n",
    "            if st == state_c and subj_idx == subject_c:\n",
    "                continue\n",
    "            covs_conscious.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    covs_nonresp = []\n",
    "    for st in nonresponsive_states[selected_dataset]:\n",
    "        covs = all_covs[selected_dataset][st]\n",
    "        for subj_idx in range(covs.shape[0]):\n",
    "            if st == state_nr and subj_idx == subject_nr:\n",
    "                continue\n",
    "            covs_nonresp.append(np.asarray(covs[subj_idx]))\n",
    "\n",
    "    n_c = len(covs_conscious)\n",
    "    n_nr = len(covs_nonresp)\n",
    "\n",
    "    if n_c == 0 or n_nr == 0:\n",
    "        raise ValueError(\n",
    "            \"Empty group after skipping discovery subject(s). Check inputs.\"\n",
    "        )\n",
    "\n",
    "    # --- Single batched tensor over ALL subjects (conscious first, then nonresponsive)\n",
    "    X_list = covs_conscious + covs_nonresp  # list of (N,N)\n",
    "    X_array = np.array(X_list)  # (D, N, N)\n",
    "    # keep dtype from numpy; move to device for GPU if requested\n",
    "    X_tensor = torch.as_tensor(X_array, device=device)\n",
    "\n",
    "    # --- HOI measures via THOI (TC, DTC, O, S)\n",
    "    measures = nplets_measures(\n",
    "        X_tensor,\n",
    "        nplets=[optimal_nplet],  # single n-plet\n",
    "        covmat_precomputed=True,\n",
    "        T=None,  # keep same behavior as your original code\n",
    "        device=device,\n",
    "        verbose=logging.WARNING,\n",
    "    )\n",
    "    # measures shape: (1, D, 4)\n",
    "    vals_hoi = measures[0, :, :4].detach().cpu().numpy()  # (D, 4)\n",
    "\n",
    "    # norm_O = O / S\n",
    "    ratio = vals_hoi[:, 2] / vals_hoi[:, 3]  # shape (D,)\n",
    "\n",
    "    # --- Classical FC: mean Fisher-z correlation within n-plet (batched, on device)\n",
    "    with torch.no_grad():\n",
    "        idx_t = torch.as_tensor(optimal_nplet, dtype=torch.long, device=device)\n",
    "        # Sub-covariance for n-plet: (D, k, k)\n",
    "        cov_sub = X_tensor.index_select(1, idx_t).index_select(2, idx_t)\n",
    "\n",
    "        # Convert covariance to correlation\n",
    "        # var: (D, k)\n",
    "        var = torch.diagonal(cov_sub, dim1=-2, dim2=-1)\n",
    "        eps = 1e-12\n",
    "        std = torch.sqrt(torch.clamp(var, min=eps))  # (D, k)\n",
    "        denom = std.unsqueeze(-1) * std.unsqueeze(-2)  # (D, k, k)\n",
    "        corr = cov_sub / torch.clamp(denom, min=eps)\n",
    "\n",
    "        # numerical safety for Fisher z\n",
    "        corr = torch.clamp(corr, -0.999999, 0.999999)\n",
    "\n",
    "        k = idx_t.numel()\n",
    "        # upper triangle mask, excluding diagonal\n",
    "        triu_mask = torch.triu(\n",
    "            torch.ones(k, k, dtype=torch.bool, device=device), diagonal=1\n",
    "        )\n",
    "        # (D, K) where K = k(k-1)/2\n",
    "        corr_pairs = corr[:, triu_mask]\n",
    "        z_pairs = torch.atanh(corr_pairs)\n",
    "        fc_mean_z_tensor = z_pairs.mean(dim=-1)  # (D,)\n",
    "\n",
    "    fc_mean_z = fc_mean_z_tensor.detach().cpu().numpy()  # (D,)\n",
    "\n",
    "    # --- Stack all features: [TC, DTC, O, S, norm_O, FC_mean_z]\n",
    "    vals = np.column_stack((vals_hoi, ratio, fc_mean_z))  # (D, 6)\n",
    "\n",
    "    # --- Labels: 1 for conscious, 0 for nonresponsive\n",
    "    y = np.concatenate([np.ones(n_c, dtype=int), np.zeros(n_nr, dtype=int)])  # (D,)\n",
    "\n",
    "    # --- ANOVA F-score across all columns\n",
    "    F_vals, _ = f_classif(vals, y)  # shape (6,)\n",
    "    # Guard against zero variance\n",
    "    for j in range(vals.shape[1]):\n",
    "        if np.allclose(vals[:, j].var(), 0):\n",
    "            F_vals[j] = 0.0\n",
    "\n",
    "    # --- PR AUC & inverse-PR AUC per column\n",
    "    pr_aucs = []\n",
    "    neg_pr_aucs = []\n",
    "    for j in range(vals.shape[1]):\n",
    "        xj = vals[:, j]\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(y, xj))\n",
    "            neg_pr_aucs.append(average_precision_score(y, -xj))\n",
    "        except ValueError:\n",
    "            pr_aucs.append(np.nan)\n",
    "            neg_pr_aucs.append(np.nan)\n",
    "\n",
    "    out_list = []\n",
    "    for jdx, measure_ in enumerate(measure_list):\n",
    "        out_list.append(\n",
    "            {\n",
    "                \"row_idx\": idx,\n",
    "                \"measure\": measure_,\n",
    "                \"F_score\": F_vals[jdx],\n",
    "                \"PR_AUC\": pr_aucs[jdx],\n",
    "                \"PR_AUC_inv\": neg_pr_aucs[jdx],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return out_list\n",
    "\n",
    "\n",
    "all_covs = load_covariance_dict(f\"{results_path}/covariance_matrices_gc.h5\")\n",
    "\n",
    "conscious_states = {\n",
    "    \"MA\": [\"MA_awake\"],\n",
    "    # \"DBS\": [\"DBS_awake\", \"ts_on_5V\"],\n",
    "    \"DBS\": [\"DBS_awake\"],\n",
    "    # \"MA_DBS\": [\"MA_awake\", \"DBS_awake\", \"ts_on_5V\"],\n",
    "}\n",
    "nonresponsive_states = {\n",
    "    \"MA\": [\"deep_propofol\", \"ketamine\", \"moderate_propofol\", \"ts_selv2\", \"ts_selv4\"],\n",
    "    \"DBS\": [\n",
    "        \"ts_off\",\n",
    "        \"ts_on_3V_control\",\n",
    "        \"ts_on_5V_control\",\n",
    "    ],\n",
    "    # \"MA_DBS\": [\n",
    "    #     \"deep_propofol\",\n",
    "    #     \"ketamine\",\n",
    "    #     \"moderate_propofol\",\n",
    "    #     \"ts_selv2\",\n",
    "    #     \"ts_selv4\",\n",
    "    #     \"ts_off\",\n",
    "    #     \"ts_on_3V_control\",\n",
    "    #     \"ts_on_5V_control\",\n",
    "    # ],\n",
    "}\n",
    "\n",
    "temp = []\n",
    "for order in [2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    temp.append(\n",
    "        pd.read_csv(\n",
    "            f\"{results_path}/N_1_A_max_O_diff_MA_{order}.csv\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            sep=\";\",\n",
    "            decimal=\",\",\n",
    "        )\n",
    "    )\n",
    "results_df_MA = pd.concat(temp).reset_index(drop=True)\n",
    "\n",
    "temp = []\n",
    "for order in [2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    temp.append(\n",
    "        pd.read_csv(\n",
    "            f\"{results_path}/N_1_A_max_O_diff_DBS_{order}.csv\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            sep=\";\",\n",
    "            decimal=\",\",\n",
    "        )\n",
    "    )\n",
    "results_df_DBS = pd.concat(temp).reset_index(drop=True)\n",
    "\n",
    "results_df_MA.value_counts([\"task\"])\n",
    "results_df_DBS.value_counts([\"task\"])\n",
    "results_df_DBS.query(\"order==5\").value_counts([\"state_c\", \"state_nr\", \"task\"])\n",
    "\n",
    "O_diff_df = pd.read_csv(\n",
    "    f\"{results_path}/N_1_A_max_O_differences_2_9.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    ")\n",
    "minmax_df = pd.read_csv(\n",
    "    f\"{results_path}/N_1_A_minmax_differences_2_9.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    ")\n",
    "\n",
    "################################################################################################################################################\n",
    "# ANOVA, Cohen's D, AUC (ROC & PR)\n",
    "################################################################################################################################################\n",
    "\n",
    "results_dict = {\n",
    "    # \"diff_OA\": O_diff_df,\n",
    "    # \"minmax\": minmax_df,\n",
    "    # \"DBS\": results_df_DBS,\n",
    "    \"DBS\": results_df_DBS,\n",
    "    # \"MA\": results_df_MA,\n",
    "}\n",
    "\n",
    "for result_name, results_df in results_dict.items():\n",
    "    eval_results = []\n",
    "    for idx, row in tqdm(\n",
    "        results_df.iterrows(),\n",
    "        total=len(results_df),\n",
    "        desc=\"Evaluating n-plets\",\n",
    "    ):\n",
    "        # metrics = evaluate_nplet_batched_both_datasets(\n",
    "        #     idx,\n",
    "        #     all_covs,\n",
    "        #     conscious_states,\n",
    "        #     nonresponsive_states,\n",
    "        #     ast.literal_eval(row[\"optimal_nplet\"]),\n",
    "        #     row[\"state_c\"],\n",
    "        #     row[\"state_nr\"],\n",
    "        #     row[\"subject_c\"],\n",
    "        #     row[\"subject_nr\"],\n",
    "        #     device=device,\n",
    "        # )\n",
    "        metrics = evaluate_nplet_batched_FC(\n",
    "            idx,\n",
    "            all_covs,\n",
    "            conscious_states,\n",
    "            nonresponsive_states,\n",
    "            result_name,\n",
    "            ast.literal_eval(row[\"optimal_nplet\"]),\n",
    "            row[\"state_c\"],\n",
    "            row[\"state_nr\"],\n",
    "            row[\"subject_c\"],\n",
    "            row[\"subject_nr\"],\n",
    "            device=device,\n",
    "        )\n",
    "        eval_results.extend(metrics)\n",
    "        if idx % 10000 == 0:\n",
    "            metrics_df = pd.DataFrame(eval_results)\n",
    "            results_eval_df = results_df.merge(\n",
    "                metrics_df,\n",
    "                left_index=True,  # results_df row index matches metrics_df row_idx groups\n",
    "                right_on=\"row_idx\",  # match on metrics_df row_idx\n",
    "            )\n",
    "            # Drop the helper column if not needed\n",
    "            results_eval_df = results_eval_df.drop(columns=[\"row_idx\"])\n",
    "            results_eval_df.to_csv(\n",
    "                f\"{results_path}/A_2_C_nplet_eval_{result_name}_AWAKE_VS_off_5vctrl_3vctrl.csv\",\n",
    "                index=False,\n",
    "                encoding=\"utf-8-sig\",\n",
    "                sep=\";\",\n",
    "                decimal=\",\",\n",
    "            )\n",
    "    metrics_df = pd.DataFrame(eval_results)\n",
    "    results_eval_df = results_df.merge(\n",
    "        metrics_df,\n",
    "        left_index=True,  # results_df row index matches metrics_df row_idx groups\n",
    "        right_on=\"row_idx\",  # match on metrics_df row_idx\n",
    "    )\n",
    "\n",
    "    # Drop the helper column if not needed\n",
    "    results_eval_df = results_eval_df.drop(columns=[\"row_idx\"])\n",
    "    results_eval_df.to_csv(\n",
    "        f\"{results_path}/A_2_C_nplet_eval_{result_name}_AWAKE_VS_off_5vctrl_3vctrl.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",\n",
    "        sep=\";\",\n",
    "        decimal=\",\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
