{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df81682e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5923b9",
   "metadata": {},
   "source": [
    "Note: This notebook should be run from the `high-order-anesthesia` folder to ensure the correct imports and file paths are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a20a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in: high-order-anesthesia\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "def ensure_project_root(target_name: str = \"high-order-anesthesia\") -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if cwd.name == target_name:\n",
    "        return cwd\n",
    "    for parent in cwd.parents:\n",
    "        if parent.name == target_name:\n",
    "            os.chdir(parent)\n",
    "            return parent\n",
    "    raise RuntimeError(\n",
    "        f\"Could not find '{target_name}' in current path or parents. \"\n",
    "        f\"Please run the notebook from inside the project.\"\n",
    "    )\n",
    "ROOT = ensure_project_root(\"high-order-anesthesia\")\n",
    "print(f\"Now in: {ROOT.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be12047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import itertools\n",
    "import logging\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82db3e4",
   "metadata": {},
   "source": [
    "Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149296e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from src.hoi_anesthesia.thoi_utils import simulated_annealing_parallel\n",
    "from src.hoi_anesthesia.utils import max_difference_pairs\n",
    "from src.hoi_anesthesia.io import load_covariance_dict, print_time\n",
    "from src.hoi_anesthesia.plotting import plot_measures_accross_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d9345",
   "metadata": {},
   "source": [
    "#### Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "data_path = \"data\"\n",
    "\n",
    "# Load covariance matrices\n",
    "all_covs = load_covariance_dict(f\"{data_path}/covariance_matrices_gc.h5\")\n",
    "\n",
    "# States for each dataset; MA: Multi-anesthesia - DBS: Deep Brain Stimulation\n",
    "conscious_states = {\n",
    "    \"MA\": [\"MA_awake\"],  \n",
    "    \"DBS\": [\"DBS_awake\", \"ts_on_5V\"],\n",
    "}\n",
    "nonresponsive_states = {\n",
    "    \"MA\": [\"ts_selv2\", \"ts_selv4\", \"moderate_propofol\", \"deep_propofol\", \"ketamine\"],\n",
    "     \"DBS\": [\"ts_off\", \"ts_on_3V_control\", \"ts_on_5V_control\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029ab67",
   "metadata": {},
   "source": [
    "#### Simulated Annealing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473d5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = 1000\n",
    "max_iter = 10000\n",
    "repeat = 100\n",
    "batch_size = 300\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58df92",
   "metadata": {},
   "source": [
    "#### Select dataset and orders to optimize \n",
    "The script saves a checkpoint csv at each order so the execution can be interrupted without losing all of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e380dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_optimize = ['DBS', 'MA']\n",
    "orders = [3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d2d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "ORDER: 3\n",
      "Evaluating DBS_awake vs ts_off with 1008 pairs for task: Cpos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n",
      "mean(max_difference_pairs) = 0.3265928019996602 - ES: 0:   0%|          | 31/10000 [00:07<40:05,  4.14it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m batched_sub_indices \u001b[38;5;241m=\u001b[39m subject_indices[\n\u001b[0;32m     36\u001b[0m     idx \u001b[38;5;241m*\u001b[39m batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m     37\u001b[0m ]\n\u001b[0;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 39\u001b[0m optimal_nplets, optimal_scores \u001b[38;5;241m=\u001b[39m \u001b[43msimulated_annealing_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlargest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_difference_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcovmat_precomputed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWARNING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m max_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(optimal_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     54\u001b[0m best_nplets \u001b[38;5;241m=\u001b[39m optimal_nplets[\n\u001b[0;32m     55\u001b[0m     max_idx, torch\u001b[38;5;241m.\u001b[39marange(optimal_nplets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     56\u001b[0m ]\n",
      "File \u001b[1;32mc:\\CAMILO\\Brain_Entropy\\REPO\\.high_order_anesthesia_repo\\lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\CAMILO\\Brain_Entropy\\REPO\\high-order-anesthesia\\src\\hoi_anesthesia\\thoi_utils.py:334\u001b[0m, in \u001b[0;36msimulated_annealing_parallel\u001b[1;34m(X, order, covmat_precomputed, T, initial_solution, repeat, batch_size, device, max_iterations, early_stop, initial_temp, cooling_rate, metric, largest, verbose)\u001b[0m\n\u001b[0;32m    328\u001b[0m _ \u001b[38;5;241m=\u001b[39m current_solution\u001b[38;5;241m.\u001b[39mscatter_(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;241m2\u001b[39m, i_sol\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), new_candidates\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    330\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Calculate energy of new solution\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# |batch_size|\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m new_energy \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate_nplets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcovmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we evaluate all items\u001b[39;49;00m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# every subset must evaluate its corresponding nplet\u001b[39;49;00m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m new_energy \u001b[38;5;241m=\u001b[39m new_energy\u001b[38;5;241m.\u001b[39mreshape(repeat, D)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m largest:\n",
      "File \u001b[1;32mC:\\CAMILO\\Brain_Entropy\\REPO\\high-order-anesthesia\\src\\hoi_anesthesia\\thoi_utils.py:216\u001b[0m, in \u001b[0;36m_evaluate_nplets\u001b[1;34m(covmats, T, batched_nplets, metric, batch_size, device)\u001b[0m\n\u001b[0;32m    210\u001b[0m metric_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    211\u001b[0m     partial(_get_string_metric, metric\u001b[38;5;241m=\u001b[39mmetric)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m metric\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# |bached_nplets| x |D| x |4 = (tc, dtc, o, s)|\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m batched_measures \u001b[38;5;241m=\u001b[39m \u001b[43mnplets_measures\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcovmats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnplets_rpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_nplets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# |batch_size|\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric_func(batched_measures)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\CAMILO\\Brain_Entropy\\REPO\\.high_order_anesthesia_repo\\lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\CAMILO\\Brain_Entropy\\REPO\\high-order-anesthesia\\src\\hoi_anesthesia\\thoi_utils.py:162\u001b[0m, in \u001b[0;36mnplets_measures\u001b[1;34m(covmats_pairs, nplets_rpo, T, device, verbose, batch_size)\u001b[0m\n\u001b[0;32m    159\u001b[0m nplets_covmats \u001b[38;5;241m=\u001b[39m nplets_covmats\u001b[38;5;241m.\u001b[39mview(bs \u001b[38;5;241m*\u001b[39m D, order, order)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Batch process all nplets at once\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tc_dtc_from_batched_covmat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnplets_covmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallmin1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbc1\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbcN\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbcNmin1\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# |curr_batch_size x D|, |curr_batch_size x D|, |curr_batch_size x D|, |curr_batch_size x D|\u001b[39;00m\n\u001b[0;32m    172\u001b[0m nplets_tc, nplets_dtc, nplets_o, nplets_s \u001b[38;5;241m=\u001b[39m m\n",
      "File \u001b[1;32mc:\\CAMILO\\Brain_Entropy\\REPO\\.high_order_anesthesia_repo\\lib\\site-packages\\thoi\\measures\\gaussian_copula.py:156\u001b[0m, in \u001b[0;36m_get_tc_dtc_from_batched_covmat\u001b[1;34m(covmats, allmin1, bc1, bcN, bcNmin1, marginal_entropies)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compute the single exclusion entropies\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# |batch_size| x |N|\u001b[39;00m\n\u001b[0;32m    155\u001b[0m single_exclusion_covmats \u001b[38;5;241m=\u001b[39m _get_single_exclusion_covmats(covmats, allmin1)\n\u001b[1;32m--> 156\u001b[0m single_exclusion_ents \u001b[38;5;241m=\u001b[39m \u001b[43m_multivariate_gaussian_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_exclusion_covmats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m bcNmin1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# |batch_size|\u001b[39;00m\n\u001b[0;32m    159\u001b[0m nplet_tc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(marginal_entropies, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m sys_ent\n",
      "File \u001b[1;32mc:\\CAMILO\\Brain_Entropy\\REPO\\.high_order_anesthesia_repo\\lib\\site-packages\\thoi\\measures\\utils.py:35\u001b[0m, in \u001b[0;36m_multivariate_gaussian_entropy\u001b[1;34m(covmats, N)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_multivariate_gaussian_entropy\u001b[39m(covmats, N):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (N\u001b[38;5;241m*\u001b[39mLOGTWOPIE \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovmats\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for selected_dataset in datasets_to_optimize:\n",
    "    t_i = time.time()\n",
    "    for order in orders:\n",
    "        results = []\n",
    "        print(\"*\" * 30)\n",
    "        print(\"ORDER:\", order)\n",
    "        # Iterate over dataset/state combinations\n",
    "        for state_c, state_nr in itertools.product(\n",
    "            conscious_states[selected_dataset], nonresponsive_states[selected_dataset]\n",
    "        ):\n",
    "            covs_c = all_covs[selected_dataset][state_c]  # shape (N_c, 82, 82)\n",
    "            covs_nr = all_covs[selected_dataset][state_nr]  # shape (N_nr, 82, 82)\n",
    "            for target_task in [\"Cpos\", \"NRpos\"]:\n",
    "                torch.cuda.empty_cache()\n",
    "                cov_list = []\n",
    "                subject_indices = []\n",
    "                for i in range(covs_c.shape[0]):\n",
    "                    for j in range(covs_nr.shape[0]):\n",
    "                        cov_c = torch.from_numpy(covs_c[i])  # 82x82\n",
    "                        cov_nr = torch.from_numpy(covs_nr[j])  # 82x82\n",
    "                        if target_task == \"Cpos\":\n",
    "                            cov_list.append(torch.stack([cov_c, cov_nr], dim=0))\n",
    "                        elif target_task == \"NRpos\":\n",
    "                            cov_list.append(torch.stack([cov_nr, cov_c], dim=0))\n",
    "                        subject_indices.append([i, j])\n",
    "\n",
    "                X = torch.stack(cov_list, dim=0).to(device)\n",
    "                n_batches = X.shape[0] // batch_size + 1\n",
    "                t_x = time.time()\n",
    "                print(\n",
    "                    f\"Evaluating {state_c} vs {state_nr} with {X.shape[0]} pairs for task: {target_task}\"\n",
    "                )\n",
    "                for idx in range(n_batches):\n",
    "                    batched_X = X[idx * batch_size : (idx + 1) * batch_size, ...]\n",
    "                    batched_sub_indices = subject_indices[\n",
    "                        idx * batch_size : (idx + 1) * batch_size\n",
    "                    ]\n",
    "                    torch.cuda.empty_cache()\n",
    "                    optimal_nplets, optimal_scores = simulated_annealing_parallel(\n",
    "                        X=batched_X,\n",
    "                        order=order,\n",
    "                        device=device,\n",
    "                        largest=True,\n",
    "                        metric=max_difference_pairs,\n",
    "                        repeat=repeat,\n",
    "                        early_stop=early_stop,\n",
    "                        max_iterations=max_iter,\n",
    "                        covmat_precomputed=True,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=logging.WARNING,\n",
    "                    )\n",
    "                    max_idx = torch.argmax(optimal_scores, dim=0)  #\n",
    "\n",
    "                    best_nplets = optimal_nplets[\n",
    "                        max_idx, torch.arange(optimal_nplets.size(1))\n",
    "                    ]\n",
    "                    best_scores = optimal_scores[\n",
    "                        max_idx, torch.arange(optimal_scores.size(1))\n",
    "                    ]\n",
    "\n",
    "                    for best_score, best_nplet, sub_indices in zip(\n",
    "                        best_scores, best_nplets, batched_sub_indices\n",
    "                    ):\n",
    "                        best_nplet = best_nplet.tolist()\n",
    "                        best_nplet.sort()\n",
    "                        dataset_c = selected_dataset\n",
    "                        dataset_nr = selected_dataset\n",
    "                        subject_c, subject_nr = sub_indices\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"order\": order,\n",
    "                                \"task\": target_task,\n",
    "                                \"state_c\": state_c,\n",
    "                                \"state_nr\": state_nr,\n",
    "                                \"subject_c\": subject_c,\n",
    "                                \"subject_nr\": subject_nr,\n",
    "                                \"optimal_nplet\": best_nplet,\n",
    "                                \"optimal_score\": best_score.item(),\n",
    "                            }\n",
    "                        )\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                results_df = pd.DataFrame(results)\n",
    "                results_df.to_csv(\n",
    "                    f\"{results_path}/R1_A_max_O_diff_{selected_dataset}_{order}.csv\",\n",
    "                    index=False,\n",
    "                    encoding=\"utf-8-sig\",\n",
    "                    sep=\";\",\n",
    "                    decimal=\",\",\n",
    "                )\n",
    "                t_y = time.time()\n",
    "                print(\n",
    "                    f\"{X.shape[0]} pairs evaluated in:\",\n",
    "                    np.round(t_y - t_x, 1),\n",
    "                    \"seconds\",\n",
    "                )\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(\n",
    "            f\"{results_path}/R1_A_max_O_diff_{selected_dataset}_{order}.csv\",\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "            sep=\";\",\n",
    "            decimal=\",\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456a66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".high_order_anesthesia_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
